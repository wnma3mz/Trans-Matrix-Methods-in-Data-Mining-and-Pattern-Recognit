第九章

聚类与非负矩阵分解

数据压缩和分类的一个重要方法是在集群中组织数据点。簇是在某些距离测量中紧密相连的一组数据点的子集。我们可以分别计算每个集群的平均值，并使用这些平均值作为集群的代表。等价地，该方法可用作基向量，并且所有数据点都用它们相对于该基的坐标表示。

​                                                  

图9.1.r3中的两个簇。

例9.1。在图9.1中，我们展示了r3中的一组数据点，这些数据点是由两个相关的正态分布生成的。假设我们知道有两个集群，我们可以很容易地从视觉上确定哪些点属于哪个类。一种聚类算法采用一组完整的点，并使用一些距离度量对它们进行分类。

一百零一

计算集群有几种方法。其中最重要的是k均值算法。我们在第9.1节中对此进行了描述。

在数据挖掘应用程序中，矩阵通常是非负的。如果我们使用SVD计算矩阵的低阶近似值，那么由于奇异向量的正交性，我们很可能得到带有负元素的因子。用负元素的低阶近似来近似非负矩阵似乎有些不自然。相反，人们通常希望用非负因子计算低阶近似值：

a≈w h，w，h≥0。（9.1）

在许多应用中，非负因子分解有助于解释应用概念中的低阶近似。

在第11章中，我们将对非负矩阵应用一种聚类算法，并将聚类中心作为基向量，即作为（9.1）中矩阵w中的列。然而，这并不能保证h也是非负的。近年来，提出了几种非负矩阵分解算法，并在不同的应用中得到了成功的应用。我们在第9.2节中描述了这种算法。

9.1 k均值算法

假设我们有n个数据点（，我们将其组织为矩阵A∈Rm×N中的列。让=（表示将向量a1，a1，…，an划分为k个簇：

πj=νaν属于簇j。假设群的平均值或质心是

，

式中，nj是πj中的元素数。我们将描述基于欧几里得距离测量的k-均值算法。

簇πj的紧密性或相干性可以用和来度量。

.

矢量越接近质心，qj值越小。聚类的质量可以用整体一致性来衡量：

.

在k-均值算法中，我们寻求一个具有最佳一致性的分区，即它是最小化问题的解。

.

9.1.k-均值算法

算法的基本思想很简单：假设有一个临时分区，就可以计算质心。然后，对于特定集群中的每个数据点，检查是否有另一个形心比当前集群形心更近。如果是这样的话，就需要重新分配。

​             

### k-均值算法

​             

\1.    从初始分区_（0）开始，计算相应的质心向量，（m（0）j）kj=1。计算q（_（0））。将t=1。

\2.    对于每个向量ai，找到最近的质心。如果最近的向量是，则指定。

\3.    计算（新分区的）质心。

\4.    如果Q（（t−1））−Q（（t））<TOL，则停止；否则，将T增加1并转到步骤2。

​             

初始分区通常是随机选择的。该算法通常具有较快的收敛速度，但不能保证算法找到全局最小值。

例9.2。从乳腺癌诊断研究[66]中选取了一个聚类分析的标准例子。基质A∈R9×683含有乳腺细胞学检查的数据。在683项检查中，444项诊断为良性，239项诊断为恶性。我们在k均值算法中迭代k=2，直到函数q（_）的相对差小于10−10。使用随机初始分区，迭代分六步收敛（参见图9.2），我们给出目标函数的值。但是需要注意的是，收敛不是单调的：第3步之后的目标函数比第6步之后的小。结果表明，在许多情况下，该算法只给出局部极小值。

由于测试数据是人工分类的，所以我们知道哪些病人患有良性肿瘤和哪些恶性肿瘤，我们可以检查算法给出的聚类。结果见表9.1。在239名恶性肿瘤患者中，K均值算法正确地将222名患者分类，但错误地将17名患者分类。

在第11章和下面的示例中，我们使用集群进行信息检索或文本挖掘。以质心向量作为基向量，用它们的坐标表示文档的基向量。

​                                                     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

















图9.2.乳腺癌数据K均值算法中的目标函数。

表9.1.用k均值算法对癌症数据进行分类。B代表良性，M代表恶性。

|      | K-表示     |            |
| ---- | ---------- | ---------- |
|      | 米         | 乙         |
| 米   | 二百二十二 | 十七       |
| 乙   | 九         | 四百三十五 |

例9.3。考虑示例1.1中的术语文档矩阵，



| A=1101000 | 零   零   一   零   一   零   一 | 一   零   零   一   一   一   一 | 零   零   零   一   零   一   零 | 0000010_______， |
| --------- | -------------------------------- | -------------------------------- | -------------------------------- | ---------------- |
|           |                                  |                                  |                                  |                  |

0 0 0 1 0_



0 0 0 0 1 0 0 0 0 1

回想一下，前四个文档涉及谷歌和网页排名，而第五个文档涉及足球。利用这一知识，我们可以将前四列向量的平均值作为该簇和第五列向量的质心。

9.1.k-均值算法

作为第二个质心，即我们使用归一化基向量

.

根据这个近似基础，A的柱坐标通过求解

.

考虑到薄qr分解c=qr，这个最小二乘问题的解h=r-1qta

.

我们看到前两列的第二基向量的坐标为负。这在术语文档设置中很难解释。例如，它意味着第一列a1近似于

.

目前还不清楚这一“近似文件”中“英格兰”和“国际足联”这两个词的含义。

最后，我们注意到，为供以后参考，相对近似误差相当高：

（9.2）

在下一节中，我们将在前面的示例中近似矩阵，确保基向量和坐标都是非负的。

9.2非负矩阵分解

给定一个数据矩阵a∈rm×n，我们要计算一个秩k近似，它被约束为具有非负因子。因此，假设w∈rm×k和h∈rk×n，我们要求解

（9.3）

同时考虑到W和H的一个优化问题，该问题是非线性的。然而，如果已知其中一个未知矩阵，比如w，那么计算h的问题将是一个标准的、非负约束的、矩阵右侧的最小二乘问题。因此，最常见的解决方法（9.3）是使用交替最小二乘法（ALS）程序[73]：

​             

### 交替非负最小二乘法

​             

\1.    猜测初始值w（1）。

\2.    对于k=1,2，…直到收敛

(a)    求解min，得到h（k）。

(b)    求解min，给出w（k+1）。

​             

然而，因式分解wh不是唯一的：我们可以引入任何带正对角元素的对角矩阵d及其因子之间的逆矩阵。

wh=（wd）（d−1h）。

为了避免一个因素的增长和另一个因素的衰退，我们需要在每次迭代中规范化其中一个因素。一种常见的规范化方法是缩放w的列，使每列中的最大元素等于1。

让aj和hj是a和h的列。逐个写出列，我们看到矩阵最小二乘问题等价于n个独立向量最小二乘问题：

   

这些问题可以通过[61，第23章]中的主动集算法来解决。通过变换矩阵，将确定w的最小二乘问题转化为m独立向量最小二乘问题。因此，ALS算法的核心可以用伪Matlab编写：

9.2.非负矩阵分解

while（not converged）[w]=规范化（w）；对于i=1:n

h（：，i）=lsqnonneg（w，a（：，i））；end for i=1:m w=lsqnonneg（h'，a（i，：）'）；w（i，：）=w'；end

结束

非负矩阵分解算法有很多变种。上述算法存在非负最小二乘主动集算法耗时的缺点。作为一种更便宜的选择，考虑到薄qr分解w=qr，我们可以采用无约束最小二乘法。

H=R−1qta，

然后将h中的所有负元素设为零，同样在算法的另一步中。在[13]中描述了增强稀疏性的改进。

[63]中给出了一个乘法算法：

while（不收敛）

w=w.%（w>=0）；

h=h.%（w'*v）./（（w'*w）*h+epsilon）；

h=h.%（h>=0）；

w=w.%（v*h’）./（w*（h*h’）+epsilon）；

[W，H]=正火（W，H）；结束

（变量epsilon应该被赋予一个小值，并用于避免被零除。）带运算符的矩阵运算。*和./等价于componentwise语句

.

该算法可以看作是一种梯度下降法。

由于非负矩阵分解法有着许多重要的应用，因此算法的发展是一个活跃的研究领域。例如，为迭代找到终止标准的问题似乎没有找到一个好的解决方案。[13]对不同算法进行了调查。

非负因子分解a≈wh可用于聚类：如果h i j是h的j列中最大的元素，则将数据向量aj赋给聚类i。

[20，37]。

非负矩阵分解法被广泛应用于：

文档聚类和电子邮件监控[85，8]，音乐转录[90]，生物信息学[20，37]和光谱分析[78]，等等。

### 9.2.1初始化

非负矩阵分解的几种算法的一个问题是不能保证收敛到全局最小值。通常情况下，收敛速度很慢，达到次优近似。计算良好初始近似值的有效程序可以基于a[18]的SVD。我们知道第一个k奇异三元组（给出了frobinius范数中a的最佳秩k近似值）。很容易看出，如果a是非负矩阵，那么u1和v1是非负矩阵（参见第6.4节）。因此，如果a=u∑v t是a的svd，我们可以把第一个奇异向量u1作为w（1）中的第一列（如果算法需要，可以把它作为初始近似h（1）中的第一行；下面我们只处理w（1）的近似值）。

由于正交性，下一个最好的向量u2很可能有负分量。但是，如果我们计算矩阵并用零替换所有的负元素，得到非负矩阵，那么我们就知道这个矩阵的第一个奇异向量是非负的。此外，我们希望它是一个相当好的u2近似值，所以我们可以把它作为w（1）的第二列。

该过程可以通过以下稍微简化的matlab脚本来实现：

[u，s，v]=svds（a，k）；%仅计算k最大奇异值

%值和相应的向量

w（：，1）=u（：，1）；对于j=2:k

c=u（：，j）*v（：，j）'；

c=c.%（c>=0）；

[U，S，V]=SVD（C，1）；

W（：，J）=U；结束

matlab[u，s，v]=svds（a，k）仅使用lanczos方法计算k最大奇异值和相应的奇异向量；见第15.8.3节。标准的SVD函数SVD（A）计算完全分解，通常速度较慢，特别是当矩阵较大且稀疏时。

例9.4.我们使用随机初始化和基于SVD的初始化计算了示例9.3中矩阵A的秩2非负因子分解。使用随机初始化时，收敛速度较慢（参见图9.3），经过10次迭代后，它没有收敛。算法的相对逼近误差

9.2.非负矩阵分解

​                                                     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     

​     













图9.3.非负矩阵因式分解中作为迭代数函数的相对近似误差。上曲线为随机初始化曲线，下曲线为基于SVD的初始化曲线。

SVD初始化为0.574（与K均值算法（9.2）的错误0.596相比）。在某些运行中，具有随机初始化的算法收敛到局部次优最小值。SVD初始化的因子分解是

.

现在可以解释分解。前四个文档由基本向量很好地表示，这些基本向量具有与Google相关的关键字的大组件。相比之下，第五个文档仅由第一个基向量表示，但其坐标比前四个面向Google的文档的坐标小。这样，排名2的近似值就突出了与Google相关的内容，而“足球文档”则被淡化了。在第11章中，我们将看到其他低阶近似，例如基于SVD的近似，具有类似的

效果。

另一方面，如果我们计算一个秩3近似值，那么我们得到

.

我们看到，现在w中的第三个向量实质上是一个“足球”基向量，而其他两个向量则表示与google相关的文档。


 


 
