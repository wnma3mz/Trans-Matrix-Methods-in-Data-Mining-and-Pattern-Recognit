第十三章

自动关键字和关键字句子提取

由于文本信息量的激增，需要开发自动的文本摘要程序。一种典型的情况是，Web搜索引擎从每个文档中显示少量与特定查询匹配的文本。另一个相关领域是新闻文章的总结。

自动文本摘要是一个活跃的研究领域，它与信息检索、自然语言处理和机器学习等多个领域有着密切的联系。非正式地，文本摘要的目标是从文本文档中提取内容，并以简明的形式和对用户或应用程序的需要敏感的方式向用户呈现最重要的内容[67]。在本章中，我们将有一个不那么雄心勃勃的目标：我们将介绍一种从文本中自动提取关键词和关键句子的方法。信息检索中的向量空间模型和pagerank概念之间存在联系。我们还将使用非负矩阵分解。本演示基于[114]。使用QR分解的文本摘要如[26，83]所述。

13.1显著性得分

考虑从中提取关键词和关键句子的文本。我们将以本书第12章为例。作为预处理步骤之一，应该执行词干化，这样具有不同词尾的相同词干只由一个标记表示。停止词（参见第11章）经常出现在文本中，但由于它们不区分不同的句子，所以应该删除它们。同样，如果文本带有特殊符号，例如数学或标记语言标记（HTML、LaTex），则可能需要删除这些符号。

由于我们想比较不同句子中的词频，我们必须将每个句子视为单独的文档（在信息检索术语中）。预处理完成后，我们使用与信息检索中相同类型的解析器来解析文本。这样，术语文档矩阵就是

一百六十一

准备好了，在本章中我们将把它称为一个词-句矩阵。因此，我们有一个矩阵a∈rm×n，其中m表示不同项的个数，n表示句子的个数。元素aij被定义为句子j中词条i的频率。

列向量（a1j a2j…在与句子j中出现的术语相对应的位置上，t不是零。同样，行向量（ai1 ai2…ain）在与包含i项的句子相对应的位置处为非零。

[114]中程序的基础是术语和句子同时但独立的排序。因此，术语i给出非负显著性得分，表示ui。显著性得分越高，这个词就越重要。句子j的显著性得分表示vj。

显著性得分的分配基于相互强化原则[114]：

如果一个词出现在许多句子中，并且有很高的显著性得分，那么它应该有很高的显著性得分。如果一个句子中包含了许多具有高显著性得分的单词，那么它应该具有高显著性得分。

更准确地说，我们认为第一项的显著性得分与出现它的句子的得分之和成正比；此外，每一项都由相应的矩阵元素加权，

​                                                  

同样，句子j的显著性得分被定义为与其词的得分成正比，并由相应的aij加权。

   

收集两个向量u∈rm和v∈rn的显著性得分，这两个方程可以写成

| σuu=平均值， | （13.1） |
| ------------ | -------- |
| σvv=atu，    | （13.2） |

其中，σu和σv是比例常数。实际上，常数必须相等。把一个方程插入另一个方程，我们得到

   

13.1.显著性得分

​             

结果表明，U和V分别是AAT和ATA的特征向量，具有相同的特征值。因此，u和v是对应于相同奇异值的奇异向量。

如果我们选择最大的奇异值，那么我们就可以保证u和v的分量是非负的。

总之，术语的显著性得分被定义为u1的组成部分，句子的显著性得分是v1的组成部分。

​                                                     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     

​     

​     

​     

​     

​     

​     









​                                                     



​     

​     





​     

​     





​     

​     



​     

​     



​     

​     





​     

​     





​     

​     





​     

​     





​     

​     





​     

​     





​     

​     



​     

​     



​     

​     



​     

​     



​     

​     



​     

​     

​     

​     

​     

​     









图13.1.第12章的显著性得分：学期得分（上）和句子得分（下）。

例13.1。我们根据第12章创建了一个术语句矩阵。由于文本是使用LaTex编写的，所以我们首先必须删除所有LaTex排版命令。这是使用一个名为detex.35的词汇扫描仪完成的，然后对文本进行词干处理并删除停止词。使用文本解析器tmg[113]构造了一个术语句矩阵A：183个句子中有388个术语。第一个奇异向量在matlab中计算，[u，s，v]=svds（a，1）。（矩阵是稀疏的，所以我们对稀疏矩阵使用SVD函数。）奇异向量如图13.1所示。

通过定位U1的10个最大组件并使用文本解析器生成的字典，我们发现以下按重要性排序的单词在本章中最重要：



γ

图13.2.秩1近似的符号说明：.

网页，搜索，大学，网络，谷歌，排名，大纲，链接，数字，相等

以下是六个最重要的句子，顺序如下：

\1.    2005年9月29日，谷歌使用搜索短语大学进行了一次搜索，结果链接到以下知名大学：哈佛大学、斯坦福大学、剑桥大学、耶鲁大学、康奈尔大学、牛津大学。

\2.    当使用搜索引擎在互联网上进行搜索时，首先是传统的文本处理部分，目的是找到包含查询词的所有网页。

\3.    不严格地说，如果一个网页有来自其他高排名网页的链接，谷歌会给它分配一个高排名。

\4.    假设一个浏览网页的浏览者从相同概率的大纲中选择下一个网页。

\5.    同样，J列的非零元素在与J的大纲对应的位置上等于NJ，并且，如果页面有大纲，则J列中所有元素的总和等于1。

\6.    附加的排名1项的随机行走解释是，在每一个时间步骤中，访问页面的冲浪者将跳转到概率为1−α的随机页面（有时称为远程传输）。

显然，这种方法更喜欢长句。另一方面，这些句子无疑是课文的关键句子。

上述方法也可被视为术语句矩阵A的秩1近似值，如图13.2所示。

在这个解释中，向量u1是a列所跨越的子空间的基向量，行向量根据这个基持有a列的坐标。现在我们看到基于显著性得分的方法有一个缺点：如果有两个包含相同显著性项的“最高级句子”，那么它们的坐标将大致相同，并且两个句子将被提取为关键句子。这是不必要的，因为它们非常相似。接下来我们将看到，如果我们将关键句子提取建立在秩k近似的基础上，就可以避免这种情况。

| 一   | γ    |      |
| ---- | ---- | ---- |
|      |      |      |

13.2.从秩k近似中提取关键句子

图13.3.低阶近似的符号说明：a≈cd。

13.2从K级中提取关键句子

近似值

假设我们已经计算出了一个很好的条件句矩阵的秩k近似值，

a≈c d，c∈rm×k，d∈rk×n，（13.3）

如图13.3所示。这种近似可以基于SVD、聚类[114]或非负矩阵因式分解。选择的维度k大于或等于要提取的关键句子数。c是基向量的秩k矩阵，d的每一列根据基向量保持a中相应列的坐标。

现在回想一下，C中的基向量表示“句子空间”中最重要的方向，即列空间。然而，低阶近似并不能立即给出最重要的句子。如果我们首先确定A列的基础是“最重的”，即D列的最大2-范数，就可以找到这些句子。这定义了一个新的基向量。然后我们继续确定d列，它是剩余k-1基向量中最重的列，依此类推。

为了推导该方法，我们注意到在近似等式（13.3）中，我们可以在c和d之间引入任何非奇异矩阵t及其逆矩阵，并且我们可以在不改变关系的情况下，将该关系与右边的任何置换p相乘：

ap≈cdp=（ct）（t−1dp）

式中t∈Rk×k。

从近似值（13.3）开始，我们首先找到d中最大范数的列，并将其按p1排列到第一列，同时将a的对应列移动到第一位置。然后我们确定一个户主转换q1，它将元素（1,1）下方第一列中的元素归零，并将转换应用于c和d：

.

事实上，这是以d为支点的QR分解的第一步，我们以m=6，n=5，k=3的例子继续讨论。

为了说明该程序处理两个或多个非常相似的重要句子的问题（参见第164页），我们还假设d的第4列与移动到第一个位置的列具有几乎相同的坐标。

在第一步之后，矩阵具有结构

，

其中，κ是DP1第一列的欧几里德长度。由于第4列与现在位于位置1的列类似，所以在第2行和第3行中有小的条目。然后我们引入对角矩阵

   

各因素之间：

.

这只改变左因子的第1列和右因子的第1列。

（标有）。从关系中

ap1≈c1d1，

我们现在看到ap1中的第一列大约等于c1中的第一列。记住原矩阵C的列是矩阵A的主方向，我们现在已经确定了A的“主列”。

在继续之前，我们进行以下观察。如果D的一列与第一列（示例中的第4列）相似，那么它现在将在第一行下面具有小元素，并且在选择第二个最主要的文档时不会起到作用。因此，如果有两个或两个以上的重要句子的关键字或多或少相同，则只选择其中一个。

接下来，我们确定A的第二个最主要的列。为此，我们计算d1列的规范，不包括第一行（因为该行根据c1的第一列保存坐标）。列和

13.2.从秩k近似中提取关键句子

​             

最大的标准移动到位置2，并通过类似于上述方式的户主转换减少。在这一步之后，我们

.

因此第二列

ap1p2≈c2d2

拥有第二大主宰地位。

继续这个过程，最终结果是

，

其中r是上三角形，p是排列的产物。现在，ap的前k列包含矩阵的支配列，这些列的秩k近似值在ck中。如果我们写的话这就更清楚了

，（13.4）

哪里

是原始C的旋转和缩放版本（即Rm中跨过相同子空间的列），它仍然保持A的主导方向。假设ai1、ai2、…、a i k是ap的前k列。则（13.4）等于

aij≈c_j，j=1,2，…，k.

这意味着由C列给出的控制方向，

与a中的k列直接关联。

上述算法相当于通过柱旋转计算QR分解（见第6.9.1节）。

，

其中q是正交的，r是上三角形。注意，如果我们只想找到前k个句子，我们不需要对基向量矩阵进行任何转换，找到前k个句子的算法可以在matlab中实现，如下所示：

%c*d是a的秩k近似值

[q，rs，p]=qr（d）；p=[1:n]*p；

p k=p（1:k）；%ap前k列的索引

例13.2。我们使用第9.2节的乘法算法计算了例13.1中的术语句矩阵的非负矩阵因子分解。然后我们用上面描述的方法确定了六个最前面的句子。选择了实施例13.1中的句子1、2、3和5，以及以下内容

二：

\1.    由于a的稀疏性和维数（几十亿阶），使用第15章中描述的密集矩阵的任何标准方法计算特征向量都是不可能的，因为这些方法是基于对矩阵应用正交变换。

\2.    在[53]中，描述了一种自适应方法，该方法检查pagerank向量各分量的收敛性，并避免对这些分量执行幂迭代。

当使用SVD计算低阶近似值时，得到了相同的结果。


 